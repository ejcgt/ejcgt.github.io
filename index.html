<!DOCTYPE html>
<html lang="en-us">

<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme-color" content="#2962ff">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
  <link rel="stylesheet" href="/academic.css">
  <title>Eunji Chong</title>
</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" >

<div class="container">

<div class="row">
  <div class="col-12 col-lg-4">
    <div id="profile">
      <img class="avatar avatar-circle" src="/profile.jpg" alt="Avatar">
      <div class="portrait-title">
        <h2>Eunji Chong, PhD</h2>
      </div>

      <ul class="network-icon" aria-hidden="true">
        <li>
          <a href="https://scholar.google.com/citations?user=Pb5N0xEAAAAJ&hl=en" target="_blank" rel="noopener">
            <i class="ai ai-google-scholar big-icon"></i>
          </a>
        </li>

        <li>
          <a href="https://github.com/ejcgt" target="_blank" rel="noopener">
            <i class="fab fa-github big-icon"></i>
          </a>
        </li>

        <li>
          <a href="https://drive.google.com/file/d/1CPM9tB7sPglFHfV2ggGH0UmAPiScO43o/view?usp=sharing" target="_blank" rel="noopener">
            <i class="ai ai-cv big-icon"></i>
          </a>
        </li>

      </ul>

    </div>
  </div>

  <div class="col-12 col-lg-8">
    <p> I am starting a new position at Amazon as Applied Scientist.
      <br>
      I received my Ph.D. from the <a href="https://ic.gatech.edu/" target="_blank" rel="noopener">School of Interactive Computing</a>
      at <a href="https://gatech.edu" target="_blank" rel="noopener">Georgia Tech</a> in 2020,
      where I worked under the supervision of Professor <a href="https://rehg.org" target="_blank" rel="noopener">James M. Rehg</a>.
      During PhD, my research has focused on developing computer vision methods for measuring and interpreting visual attention in social contexts,
      in order to better model and understand human behavior.
      </p>

  <div class="row">
    <div class="col-md-6">
      <h3>Education</h3>
      <ul class="ul-edu fa-ul">

        <li>
          <i class="fa-li fas fa-graduation-cap"></i>
          <div class="description">
            <p class="course">Ph.D. in Computer Science, 2020</p>
            <p class="institution">Georgia Institute of Technology</p>
          </div>
        </li>

        <li>
          <i class="fa-li fas fa-graduation-cap"></i>
          <div class="description">
            <p class="course">B.S. in Computer Science, 2012</p>
            <p class="institution">Yonsei University</p>
          </div>
        </li>

        </ul>
      </div>
    </div>

  </div>
</div>

</div>


<section style="padding: 20px 0 20px 0;">
  <div class="container">
    <div class="row">
        <div class="col-lg-12">
          <h1>Selected Publications</h1>
          <h2>For a complete list, refer to my CV or Google Scholar page linked above</h2>
        </div>

        <div class="row">
        	<ul class="ul-papers">
            <li>
              <div class="description">
                <p class="title">Detecting Attended Visual Targets in Video</p>
                <p class="authors"><u>Eunji Chong</u>, Yongxin Wang, Nataniel Ruiz, James M. Rehg</p>
                <p class="venue">CVPR 2020 <span style="font-weight:bolder;color:#BB2222"></span></p>
                <p class="resources">
                  [ <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Chong_Detecting_Attended_Visual_Targets_in_Video_CVPR_2020_paper.pdf" target="_blank" rel="noopener">paper</a> ]
                  [ <a href="https://github.com/ejcgt/attention-target-detection" target="_blank" rel="noopener">code</a> ]
                  [ <a href="https://github.com/ejcgt/attention-target-detection#dataset-1" target="_blank" rel="noopener">dataset</a> ]
                  [ <a href="papers/cvpr20bib.html" target="_blank" rel="noopener">bibtex</a> ]
                </p>
              </div>
            </li>

            <li>
              <div class="description">
                <p class="title">Detection of eye contact with deep neural networks is as accurate as human experts</p>
                <p class="authors"><u>Eunji Chong</u>, et. al.</p>
                <p class="venue">Nature Communications<span style="font-weight:bolder;color:#BB2222"></span></p>
                <p class="resources">
                  [ <a href="https://nature-research-under-consideration.nature.com/posts/60730-detection-of-eye-contact-with-deep-neural-networks-is-as-accurate-as-human-experts" target="_blank" rel="noopener">paper</a> ]
                  [ <a href="https://github.com/ejcgt/eye-contact-cnn" target="_blank" rel="noopener">code</a> ]
                  [ <a href="papers/natcommbib.html" target="_blank" rel="noopener">bibtex</a> ]
                </p>
              </div>
            </li>

            <li>
              <div class="description">
                <p class="title">Connecting Gaze, Scene, and Attention: Generalized Attention Estimation via Joint Modeling of Gaze and Scene Saliency</p>
                <p class="authors"><u>Eunji Chong</u>, Nataniel Ruiz, Yongxin Wang, Yun Zhang, Agata Rozga, James M. Rehg</p>
                <p class="venue">ECCV 2018 <span style="font-weight:bolder;color:#BB2222"></span></p>
                <p class="resources">
                  [ <a href="https://openaccess.thecvf.com/content_ECCV_2018/papers/Eunji_Chong_Connecting_Gaze_Scene_ECCV_2018_paper.pdf" target="_blank" rel="noopener">paper</a> ]
                  [ <a href="https://github.com/ejcgt/attention-target-detection#dataset" target="_blank" rel="noopener">annotation</a> ]
                  [ <a href="papers/eccv18bib.html" target="_blank" rel="noopener">bibtex</a> ]
                </p>
              </div>
            </li>

            <li>
              <div class="description">
                <p class="title">Fine-Grained Head Pose Estimation Without Keypoints</p>
                <p class="authors">Nataniel Ruiz, <u>Eunji Chong</u>, James M. Rehg</p>
                <p class="venue">Workshop on Analysis and Modeling of Faces and Gestures at CVPR 2018<span style="font-weight:bolder;color:#BB2222"></span></p>
                <p class="resources">
                  [ <a href="https://openaccess.thecvf.com/content_cvpr_2018_workshops/papers/w41/Ruiz_Fine-Grained_Head_Pose_CVPR_2018_paper.pdf" target="_blank" rel="noopener">paper</a> ]
                  [ <a href="https://github.com/natanielruiz/deep-head-pose" target="_blank" rel="noopener">code</a> ]
                  [ <a href="papers/cvprw18bib.html" target="_blank" rel="noopener">bibtex</a> ]
                </p>
              </div>
            </li>

        	</ul>
        </div>

    </div>
  </div>
</section>


</body>
</html>
